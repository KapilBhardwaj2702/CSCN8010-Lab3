{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34e8e6e",
   "metadata": {},
   "source": [
    "# Lab -3 - CSCN8010\n",
    "\n",
    "# Dogs vs Cats Classification: Fine-Tuning Pretrained Models\n",
    "\n",
    "**Purpose:**  \n",
    "Fine-tune a pre-trained VGG16 model and train a custom CNN on a subset of the Dogs vs Cats dataset. Evaluate and compare their performances.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Data Preparation\n",
    "\n",
    "We will download the dataset, extract 5,000 images balanced between cats and dogs, and prepare data loaders.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d5ee056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fb1bfd",
   "metadata": {},
   "source": [
    "### Download and Prepare Subset of Dogs vs Cats Dataset\n",
    "\n",
    "**Note:**  \n",
    "- The full dataset has 25,000 images (12,500 cats and 12,500 dogs).  \n",
    "- For this lab, we use only 5,000 images (2,500 cats + 2,500 dogs).  \n",
    "- Organize images into train and validation directories with balanced classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68caebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# List first 10 files in Data/train/cats (example)\n",
    "train_cats_dir = './Data/train/cats'\n",
    "files = os.listdir(train_cats_dir)\n",
    "print(files[:10])  # first 10 cat images\n",
    "\n",
    "# Similarly, for dogs\n",
    "train_dogs_dir = './Data/train/dogs'\n",
    "files = os.listdir(train_dogs_dir)\n",
    "print(files[:10])  # first 10 dog images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826c522",
   "metadata": {},
   "source": [
    "## Displaying the total of Images division for Cats and Dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cf1db86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set counts: {'cats': 0, 'dogs': 0}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the train directory path\n",
    "train_dir = './Data/train'\n",
    "\n",
    "# Function to count images in cats and dogs folders\n",
    "def count_images(directory):\n",
    "    counts = {}\n",
    "    for category in ['cats', 'dogs']:\n",
    "        category_dir = os.path.join(directory, category)\n",
    "        counts[category] = len(os.listdir(category_dir))\n",
    "    return counts\n",
    "\n",
    "# Get counts for train folder\n",
    "train_counts = count_images(train_dir)\n",
    "print(\"Train set counts:\", train_counts)\n",
    "\n",
    "# Plot the counts as bar chart\n",
    "categories = list(train_counts.keys())\n",
    "counts = list(train_counts.values())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(categories, counts, color=['orange', 'skyblue'])\n",
    "plt.title('Number of Images per Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392ca9f",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's look at some sample images and the distribution of classes. I have changes the resolution of all the images to a common resolution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80df42d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample larger than population or is negative",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     24\u001b[39m     plt.show()\n\u001b[32m     26\u001b[39m train_dir = \u001b[33m'\u001b[39m\u001b[33m./Data/train\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mplot_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m plot_samples(train_dir, \u001b[33m'\u001b[39m\u001b[33mdogs\u001b[39m\u001b[33m'\u001b[39m, target_size=(\u001b[32m150\u001b[39m, \u001b[32m150\u001b[39m), n_samples=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mplot_samples\u001b[39m\u001b[34m(directory, category, target_size, n_samples)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mplot_samples\u001b[39m(directory, category, target_size=(\u001b[32m150\u001b[39m, \u001b[32m150\u001b[39m), n_samples=\u001b[32m5\u001b[39m):\n\u001b[32m     14\u001b[39m     category_dir = os.path.join(directory, category)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     sample_images = \u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategory_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     plt.figure(figsize=(\u001b[32m15\u001b[39m, \u001b[32m3\u001b[39m))\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, img_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sample_images):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\random.py:430\u001b[39m, in \u001b[36mRandom.sample\u001b[39m\u001b[34m(self, population, k, counts)\u001b[39m\n\u001b[32m    428\u001b[39m randbelow = \u001b[38;5;28mself\u001b[39m._randbelow\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= k <= n:\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSample larger than population or is negative\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    431\u001b[39m result = [\u001b[38;5;28;01mNone\u001b[39;00m] * k\n\u001b[32m    432\u001b[39m setsize = \u001b[32m21\u001b[39m        \u001b[38;5;66;03m# size of a small set minus size of an empty list\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Sample larger than population or is negative"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def resize_image(img_path, target_size=(150, 150)):\n",
    "    img = Image.open(img_path)\n",
    "    if img.size != target_size[::-1]:  # PIL size is (width, height)\n",
    "        img = img.resize(target_size)\n",
    "    return np.array(img)\n",
    "\n",
    "def plot_samples(directory, category, target_size=(150, 150), n_samples=5):\n",
    "    category_dir = os.path.join(directory, category)\n",
    "    sample_images = random.sample(os.listdir(category_dir), n_samples)\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, img_name in enumerate(sample_images):\n",
    "        img_path = os.path.join(category_dir, img_name)\n",
    "        img = resize_image(img_path, target_size)\n",
    "        plt.subplot(1, n_samples, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(category.capitalize())\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "train_dir = './Data/train'\n",
    "\n",
    "plot_samples(train_dir, 'cats', target_size=(150, 150), n_samples=5)\n",
    "plot_samples(train_dir, 'dogs', target_size=(150, 150), n_samples=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c52253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set counts: {'cats': 2000, 'dogs': 2000}\n",
      "Validation set counts: {'cats': 500, 'dogs': 500}\n"
     ]
    }
   ],
   "source": [
    "# Check counts per category for train and validation sets\n",
    "def count_images(directory):\n",
    "    counts = {}\n",
    "    for category in ['cats', 'dogs']:\n",
    "        category_dir = os.path.join(directory, category)\n",
    "        counts[category] = len(os.listdir(category_dir))\n",
    "    return counts\n",
    "\n",
    "print(\"Train set counts:\", count_images(train_dir))\n",
    "validation_dir = './Data/validation'  \n",
    "\n",
    "print(\"Validation set counts:\", count_images(validation_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9fd174",
   "metadata": {},
   "source": [
    "## 3. Data Generators with Augmentation\n",
    "\n",
    "We use `ImageDataGenerator` for training (with augmentation) and validation (only rescaling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38764639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "image_size = (150, 150)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    shuffle=False)  # shuffle=False for consistent evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930a64e9",
   "metadata": {},
   "source": [
    "## 4. Define and Train a Custom CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c0b909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">148</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">74</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">72</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">34</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36992</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,940,416</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">513</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m148\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m74\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m72\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m34\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36992\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │    \u001b[38;5;34m18,940,416\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m513\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,034,177</span> (72.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,034,177\u001b[0m (72.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define simple CNN architecture\n",
    "def create_custom_cnn():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        layers.MaxPooling2D(2,2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "custom_cnn = create_custom_cnn()\n",
    "custom_cnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94b771c",
   "metadata": {},
   "source": [
    "### Train Custom CNN with EarlyStopping and ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0e4446",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\n",
      "Epoch 1: val_accuracy improved from -inf to 0.00000, saving model to fast_custom_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 77s - 3s/step - accuracy: 0.5104 - loss: 0.8158 - val_accuracy: 0.0000e+00 - val_loss: 0.8328\n",
      "Epoch 2/20\n",
      "\n",
      "Epoch 2: val_accuracy improved from 0.00000 to 0.74375, saving model to fast_custom_cnn.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 - 63s - 2s/step - accuracy: 0.5792 - loss: 0.6763 - val_accuracy: 0.7437 - val_loss: 0.6606\n",
      "Epoch 3/20\n",
      "\n",
      "Epoch 3: val_accuracy did not improve from 0.74375\n",
      "30/30 - 53s - 2s/step - accuracy: 0.5615 - loss: 0.6815 - val_accuracy: 0.4531 - val_loss: 0.8299\n",
      "Epoch 4/20\n",
      "\n",
      "Epoch 4: val_accuracy did not improve from 0.74375\n",
      "30/30 - 69s - 2s/step - accuracy: 0.5802 - loss: 0.6717 - val_accuracy: 0.7437 - val_loss: 0.6616\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\epoch_iterator.py:116: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_accuracy did not improve from 0.74375\n",
      "30/30 - 10s - 349ms/step - accuracy: 0.5625 - loss: 0.6694 - val_accuracy: 0.5594 - val_loss: 0.7180\n",
      "Epoch 6/20\n",
      "\n",
      "Epoch 6: val_accuracy did not improve from 0.74375\n",
      "30/30 - 60s - 2s/step - accuracy: 0.5969 - loss: 0.6625 - val_accuracy: 0.3250 - val_loss: 0.7894\n",
      "Epoch 7/20\n",
      "\n",
      "Epoch 7: val_accuracy did not improve from 0.74375\n",
      "30/30 - 54s - 2s/step - accuracy: 0.6229 - loss: 0.6464 - val_accuracy: 0.7094 - val_loss: 0.6392\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 2.\n"
     ]
    }
   ],
   "source": [
    "# Save best model only\n",
    "checkpoint_path_cnn = 'fast_custom_cnn.h5'\n",
    "checkpoint_cnn = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path_cnn,\n",
    "    monitor='val_accuracy',\n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Early stop earlier (after 2 epochs of no improvement)\n",
    "earlystop_cnn = callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history_cnn = custom_cnn.fit(\n",
    "    train_generator,\n",
    "    epochs=20, \n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_cnn, earlystop_cnn],\n",
    "    steps_per_epoch=30,               # Optional: lower if you have many batches\n",
    "    validation_steps=10,              # Optional: reduce validation computation\n",
    "    verbose=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22fad33",
   "metadata": {},
   "source": [
    "## 5. Fine-Tune Pretrained VGG16 Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cbff34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m14,714,688\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8192\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m2,097,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m257\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,812,353</span> (64.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,812,353\u001b[0m (64.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,097,665</span> (8.00 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,097,665\u001b[0m (8.00 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load VGG16 without top, freeze base layers initially\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "base_model.trainable = False\n",
    "\n",
    "model_vgg = models.Sequential([\n",
    "    base_model,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_vgg.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_vgg.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b366a",
   "metadata": {},
   "source": [
    "### Train Top Layers on Frozen Base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2ef38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.6642 - loss: 0.5938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1310s\u001b[0m 10s/step - accuracy: 0.6649 - loss: 0.5931 - val_accuracy: 0.8660 - val_loss: 0.3173\n",
      "Epoch 2/10\n",
      "\u001b[1m 95/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m4:02\u001b[0m 8s/step - accuracy: 0.8266 - loss: 0.3886"
     ]
    }
   ],
   "source": [
    "checkpoint_path_vgg = 'best_vgg16.h5'\n",
    "checkpoint_vgg = callbacks.ModelCheckpoint(checkpoint_path_vgg, monitor='val_accuracy', save_best_only=True)\n",
    "earlystop_vgg = callbacks.EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
    "\n",
    "history_vgg = model_vgg.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_vgg, earlystop_vgg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3037df",
   "metadata": {},
   "source": [
    "### Unfreeze Some Base Layers and Continue Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7add93ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - accuracy: 0.8830 - loss: 0.2671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m638s\u001b[0m 5s/step - accuracy: 0.8830 - loss: 0.2671 - val_accuracy: 0.9150 - val_loss: 0.2047\n",
      "Epoch 2/10\n",
      "\u001b[1m 25/125\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9:13\u001b[0m 6s/step - accuracy: 0.9149 - loss: 0.2089"
     ]
    }
   ],
   "source": [
    "# Unfreeze last 4 convolutional layers in VGG16\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_vgg.compile(optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "history_vgg_ft = model_vgg.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[checkpoint_vgg, earlystop_vgg])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91076e7d",
   "metadata": {},
   "source": [
    "## 6. Evaluation and Comparison of Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0268b359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best weights\n",
    "custom_cnn.load_weights(checkpoint_path_cnn)\n",
    "model_vgg.load_weights(checkpoint_path_vgg)\n",
    "\n",
    "# Evaluate accuracy\n",
    "loss_cnn, acc_cnn = custom_cnn.evaluate(validation_generator)\n",
    "loss_vgg, acc_vgg = model_vgg.evaluate(validation_generator)\n",
    "\n",
    "print(f\"Custom CNN Accuracy: {acc_cnn:.4f}\")\n",
    "print(f\"Fine-tuned VGG16 Accuracy: {acc_vgg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0336d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and true labels\n",
    "validation_generator.reset()\n",
    "y_true = validation_generator.classes\n",
    "y_pred_cnn = (custom_cnn.predict(validation_generator) > 0.5).astype(\"int32\")\n",
    "validation_generator.reset()\n",
    "y_pred_vgg = (model_vgg.predict(validation_generator) > 0.5).astype(\"int32\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48b387",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff2abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Cat', 'Dog'], yticklabels=['Cat', 'Dog'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion(y_true, y_pred_cnn, 'Custom CNN Confusion Matrix')\n",
    "plot_confusion(y_true, y_pred_vgg, 'Fine-tuned VGG16 Confusion Matrix')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15964911",
   "metadata": {},
   "source": [
    "### Classification Report (Precision, Recall, F1-score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d1c2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Custom CNN Classification Report\")\n",
    "print(classification_report(y_true, y_pred_cnn, target_names=['Cat', 'Dog']))\n",
    "\n",
    "print(\"Fine-tuned VGG16 Classification Report\")\n",
    "print(classification_report(y_true, y_pred_vgg, target_names=['Cat', 'Dog']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508b0ff7",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14203df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_precision_recall(y_true, y_scores, model_name):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    ap = average_precision_score(y_true, y_scores)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(recall, precision, label=f'AP={ap:.3f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'Precision-Recall Curve: {model_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Predict probabilities for better PR curve\n",
    "validation_generator.reset()\n",
    "y_scores_cnn = custom_cnn.predict(validation_generator).ravel()\n",
    "validation_generator.reset()\n",
    "y_scores_vgg = model_vgg.predict(validation_generator).ravel()\n",
    "\n",
    "plot_precision_recall(y_true, y_scores_cnn, \"Custom CNN\")\n",
    "plot_precision_recall(y_true, y_scores_vgg, \"Fine-tuned VGG16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bcc48a",
   "metadata": {},
   "source": [
    "## 7. Analysis of Failure Cases\n",
    "\n",
    "Let's explore some images where the models predicted incorrectly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db8ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_misclassified_images(generator, y_true, y_pred, title, n=5):\n",
    "    category_map = {0: 'Cat', 1: 'Dog'}\n",
    "    misclassified_idxs = np.where(y_true != y_pred)[0]\n",
    "    plt.figure(figsize=(15,3))\n",
    "    for i, idx in enumerate(misclassified_idxs[:n]):\n",
    "        img_path = generator.filepaths[idx]\n",
    "        img = plt.imread(img_path)\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {category_map[y_true[idx]]}\\nPred: {category_map[y_pred[idx][0]]}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "show_misclassified_images(validation_generator, y_true, y_pred_cnn, \"Custom CNN Misclassifications\")\n",
    "show_misclassified_images(validation_generator, y_true, y_pred_vgg, \"Fine-tuned VGG16 Misclassifications\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7caff72",
   "metadata": {},
   "source": [
    "\n",
    "## Final Conclusion\n",
    "\n",
    "In this lab, I built and compared two models to classify images of dogs and cats — a custom CNN designed from scratch, and a fine-tuned version of the pre-trained **VGG16** model from ImageNet.\n",
    "\n",
    "I worked with a subset of 5,000 images (2,500 cats and 2,500 dogs). Although this was a smaller sample compared to the full dataset, it was still enough to experiment with model training, evaluation, and fine-tuning.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "* The **fine-tuned VGG16** performed significantly better than my custom CNN in terms of accuracy, precision, recall, and F1-score. This was expected, since VGG16 had already been trained on a massive dataset and was able to extract high-quality features from the images.\n",
    "* My **custom CNN**, while simpler and faster to train, didn’t generalize as well. It struggled more with subtle visual differences — like identifying blurry dogs or cats in odd poses.\n",
    "\n",
    "### What Went Well\n",
    "\n",
    "* **Transfer learning** proved extremely useful. I didn’t have to train the entire VGG16 from scratch — just added and trained a few layers on top. This saved time and led to better performance.\n",
    "* Adding **callbacks** like `EarlyStopping` and `ModelCheckpoint` helped automatically select the best model version and avoid overfitting.\n",
    "* I evaluated both models using confusion matrices and precision-recall curves, which gave me deeper insight into how they performed beyond simple accuracy.\n",
    "\n",
    "### Where the Models Struggled\n",
    "\n",
    "Even the better model made mistakes. Most misclassifications occurred when:\n",
    "\n",
    "* The animals were in non-standard poses (e.g., curled up or lying sideways).\n",
    "* The image resolution was low or the lighting was poor.\n",
    "* The background was too busy or distracting.\n",
    "\n",
    "These are understandably tricky cases — even humans might pause before deciding.\n",
    "\n",
    "### How I Could Improve the Models\n",
    "\n",
    "* **Use more data**: Training on the full 25,000-image dataset would likely improve model generalization, especially for the custom CNN.\n",
    "* **Increase training epochs**: I limited training for speed, but adding a few more epochs could enhance performance, especially with early stopping in place.\n",
    "* **Try newer architectures**: Models like ResNet or EfficientNet might outperform VGG16 and be worth exploring in future iterations.\n",
    "* **Fine-tune more layers**: In this lab, I unfroze only a few of the VGG16 layers. Unfreezing more layers could further adapt the model to this dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### Final Thoughts\n",
    "\n",
    "This lab was a valuable learning experience in **transfer learning and model evaluation**. I saw first-hand how leveraging a pre-trained model like VGG16 can significantly improve results, even with limited data, and how important it is to evaluate models carefully using multiple metrics and visual tools.\n",
    "\n",
    "There’s still room for improvement, but this provided a solid foundation for future deep learning projects.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
